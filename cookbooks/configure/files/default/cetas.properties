#
# Cetas node personality definition and functional configuration belongs here
# Functional configuration includes for all functions
#
# Comments start with #
#
# This file includes engineering development and QA specific account security
# details for operating in AWS
#

#
# Installation name & location, this is used in building subject of the support
# email notifications
#
infrastructure.installation.name=CetasEng
infrastructure.installation.location=

cloudservice.deployment.mode=false
#
# S3 keys for Eng AWS account
#
infrastructure.accesskey=
infrastructure.secretkey=

#
# SVC locator is ZK in this case
#
infrastructure.svclocator.basepath=/cetas
infrastructure.svclocator.port=2181

infrastructure.service.analytics.isenabled=true
infrastructure.service.analytics.datastore.isenabled=true
infrastructure.service.analytics.aggregates.isenabled=true
infrastructure.service.analytics.aggregates.exportinterval=20
infrastructure.service.analytics.aggregates.exporthreadscount=5
infrastructure.service.analytics.aggregates.flushinterval=3000
# posible values for compression: none|snappy|gz|lzo|lz4
infrastructure.service.analytics.database.compression=none
infrastructure.service.analytics.queue.read.timeout=100
infrastructure.messaging.usecompressedschema=true

#
# the following 'personalities'  must be enabled to operate in lab environment
# Example: local grand uinode insights feed3 feed3 hbasezk bktprefix
# -- in single node case, we will just use Cetas ZK for hbase also
#
infrastructure.personalities=local grand 192.168.33.10 192.168.33.10 192.168.33.10 192.168.33.10

io.s3.download.rootdir=/tmp
#
# Allowed values for the library name are AWS and JETS3T
#
io.s3.library=AWS
#
# S3 hostname (the default value is s3.amazonaws.com)
#
io.s3.endpoint =

#
# HDFS endpoint (in case MGMT API is pointed to HDFS)
# The URI should always point to a directory
# The directory should exist and should have appropriate permissions
# example : hdfs://demo:9000/user/paadmin/cetas/
#
io.hdfs.endpoint=
#
# Hadoop input split calculation policy and size. The policy can be
# - FIXED_BYTE (fixed byte size splits, ending in any byte)
# - FIXED_LINE (fixed number of lines per split, ending in a line)
# - BYTE_LINE (approximately fixed byte size splits, ending in a line)
# The (max) size denotes the number of lines for the FIXED_LINE policy,
# or the number of bytes in the other two policies. Both values need to
# be set properly, otherwise BYTE_LINE at 64000000 (64 MB) is assumed.
io.hadoop.input.split.policy=BYTE_LINE
io.hadoop.input.max.split.size=64000000


#
# Following parameter applicable to data processing in
# hadoop, defines number of schemas in batch submitted to
# various consumers, do not change unless you understand it.
io.hadoop.schema.consumers.batchsize=500

#
# Common java logging properties file location
#
logging.properties.file=/opt/cetas/config/log4j.properties
logging.properties.serialization.name =
logging.context.serialization.name =
logging.context.resolve = true

#
# Tenant policy for different account type: 'free', 'paid'
#
# datausage is in MB
management.tenantpolicy.default.service.days=365
management.tenantpolicy.default.warning.days=30
management.tenantpolicy.default.datausage.capacity.check=true
management.tenantpolicy.free.datausage.capacity.total=1024
management.tenantpolicy.free.datausage.capacity.month=800
management.tenantpolicy.free.datausage.stopthreshold=105
management.tenantpolicy.free.billing.type=max
management.tenantpolicy.free.enforcement.type=hard
management.tenantpolicy.paid.datausage.capacity.total=1048576
management.tenantpolicy.paid.datausage.capacity.month=1048576
management.tenantpolicy.paid.datausage.stopthreshold=115
management.tenantpolicy.paid.billing.type=monthly
management.tenantpolicy.paid.enforcement.type=soft
management.tenantpolicy.active.user.count=1000

#
# New on-premise service offering
#
# service starting date, e.g. 12/01/2013
management.tenantpolicy.onpremise.service.start=08/01/2013
# Default = 1095, 3 years
management.tenantpolicy.onpremise.service.days=1095
# percentage need sending waring email
management.tenantpolicy.onpremise.warning.percentage=80
# unit GB, used to indicate capacity
management.tenantpolicy.onpremise.datausage.capacity=2048

#
# Tenant policy for job concurrency
#
management.tenantpolicy.free.concurrency.count=2

#
# SSH key to use in password less access to cluster nodes
#
management.sshkeyfile=~/.ssh/cetas_nopass
#
# management.host is used to know the node to which feed notifications sent
#
management.host=192.168.33.10:8080
#
# management.domainip defines publicly accessible IP or DNS name
#
management.domainip=192.168.33.10

webfeeds.endpoint.url=
#
management.datastore.type=io
management.datastore.media.system.containername=cetassystem
#
# tenantbucket holds all tenant specfic hierarchical configuration
# e.g. user/<user>/tenant/<tenant>/project/<project>/source/<sourcetype>/<name>
#
management.datastore.media.s3.tenantbucket=cetastenants
#
# tenant data bucket holds all tenant specfic data information
# The hierarchy remains the same to avoid name space collisions
# e.g. user/<user>/tenant/<tenant>/project/<project>/source/<sourcetype>/<name>
#
management.datastore.media.s3.tenantbucketdata=cetastenantsdata
#
# bucket to store search migration and archived shards
# The hierarchy remains the same to avoid name space collisions
# e.g. user/<user>/tenant/<tenant>/project/<project>/source/<sourcetype>/<name>
#
management.datastore.media.s3.searchstorebucket=searchstorebucket
#
# WellKnownLocation for internally sharing information across cluster wherever
# required. Check feed-discovery.sh for usage
#
management.datastore.media.s3.syncbucket=cetasWKL
#
# Location where white / black lists are maintained for access control
#
management.datastore.media.s3.opsbucket=testops
#
# location where operational logs are collected to for analysis
#
management.datastore.media.s3.debugbucket=cetasdebug
#
# this is applicable for 'fs' type environments only
#
management.datastore.media.fs.rootdir=/var/cetas
#
# Operating object store type - S3, hdfs or FS
#
management.datastore.media.type=fs
#
# Operating installation type - aws, openstack, vcloud, fs
#
management.datastore.installation.type=fs
#
# Feed notifications disabled in isolated / perf test environment
# this avoids involving the rest of the work flow
#
management.feednotification.disabled=false
#
# Notification type: http or msgq
#
management.notification.http=
management.job.notification.url.prefix=CetasRequestBroker?action=jobstatustracker.get_jobstatus&jobstatustracker.jobStatus=$jobStatus&jobstatustracker.jobInstanceId=

#
# HTTPS requires session validation during every request processing
#
management.sessionvalidationrequired=true

#
# Upload Manager ThreadPool size
#
management.upload.active.threads=1

#
# get datasource UID from HBASE time out in mill seconds
#
management.datasource.uid.timeout=10000

#
# AMQ specific definitions
#
messaging.msgbroker=activemq
messaging.msgbroker.port=61616
messaging.msgbroker.type=activemq
messaging.msgbroker.notificationqueue.name=NotificationQueue?persistent=true;prefetchsize=1
messaging.msgbroker.systemtopic.name=SystemTopic
messaging.msgbroker.shardmigrationtopic.name=ShardMigrationTopic
messaging.msgbroker.dataeventqueue.name=DataEventQueue
messaging.msgbroker.schemaqueue.name=SchemaQueue
messaging.msgbroker.analyticsqueue.name=AnalyticsQueue?prefetchsize=1
messaging.msgbroker.indexerqueue.name=IndexerQueue?prefetchsize=1
messaging.msgbroker.batchprocqueue.name=BatchProcessingQueue?persistent=true;prefetchsize=1
messaging.msgbroker.datafeedqueue.name=DataFeedProcessingQueue?persistent=true;prefetchsize=1
messaging.msgbroker.statsqueue.name=StatisticsQueue
messaging.msgbroker.jobstatusqueue.name=JobStatusQueue
messaging.msgbroker.crawlqueue.name=CrawlQueue
messaging.msgbroker.usernotificationtopic.name=UserNotificationTopic
messaging.msgbroker.previewqueue.name=DataPreviewQueue
messaging.dynamic.queues.pendingmessages.limit=100
messaging.flowcontrol.storage.percentage.limit=20
## The absolute limit is specified in GB
messaging.flowcontrol.storage.absolute.limit=2
## the inactivity time limit of dynamic queues is specified in seconds
messaging.dynamic.queues.inactivity.timelimit=604800

#
# Cetas Indexer specific configuration variables
#
indexer.solr.basedir.name=/opt/apache/solr/example/cetas
indexer.solr.schema.filename=schema.xml
indexer.solr.config.filename=solrconfig.xml
indexer.solr.cores.filename=solr.xml
indexer.solr.master.core.configuration.dir=/opt/apache/solr/example/cetas/MASTER-CORE/conf

###
# Index server constants
# Changing these properties will impact performance of the indexer server
###
indexer.server.num.consumers=5
indexer.server.commiteverybatch=false
indexer.server.streaming.client.queuesize=200
indexer.server.streaming.client.threadcount=3

# Size of the shard in MB
indexer.index.shard.max.size=100
indexer.index.shard.max.duration=1d
indexer.index.distributed=false
indexer.index.maxidletime=1800
# max time SOLR wait before autocommits uncommited documents
indexer.solr.flush.timeout=5000
# number of shards for paid user
index.num.partition.for.paid.account.cubes=8

###
# Search node constants
###
searcher.ebs.index.mountpath=/var/cetasdata/index/
searcher.index.migration.tmppath=/var/cetasdata/migrated_index/
searcher.fs.volume.path=/var/volumes
searcher.unmount.volume=false
searcher.imported.shard.batchcount=10
# This size is in MB
searcher.mapreduce.inmem.shardsize=64
searcher.query.streaming=false
searcher.streaming.domain.name=

###
# These limits are for the query SHARDS and not the index shards
###
searcher.shard.sizelimit=4096
searcher.shard.timelimit=1d
searcher.shard.merge=true
searcher.shard.force.sync.duration=30
searcher.volume.sizeInGB=40

###
# When enabled the facets generated for analytics will be based on the data type
# for example, all integers will be generated with range faceting
###
schema.facet.detecttype=true
schema.datastore.media.type=fs
schema.max_count_tillpersist=100000

#
# Tools invoked by java interfaces to apply live feed and cetasbox confgs
#
feed.feedagent.buildagent.script=/opt/cetas/bin/build-cetas-agent.sh
feed.feedagent.removeagent.script=/opt/cetas/bin/remove_cetasbox_channel.py
feed.feedagent.masterconfiguration.script=/opt/cetas/bin/add_cetasbox_channel.py
feed.feedagent.healthcheck.module=/opt/cetas/bin/channel-health.sh
feed.syslog.removeagent.script=/opt/cetas/bin/remove-syslog-channel.sh
feed.syslog.masterconfiguration.script=/opt/cetas/bin/add-syslog-channel.sh
#
# Port ranges used by PAT for distinct functionalities
#
feed.cetasbox.ports.lowerbound=13000
feed.cetasbox.ports.upperbound=14000
feed.feedagent.ports.lowerbound=14001
feed.feedagent.ports.upperbound=35000
#
# Only Names will work for flume configurations. These are resolved during
# personality assignment and updated in this file. Run time configuration may
# show these entries in a different location of the file due to auto-discovery
# script meddling with them.
#
feed.cetasbox.master.nodeip=192.168.33.10
feed.cetasbox.collector.nodeip=192.168.33.10
feed.feedagent.master.nodeip=192.168.33.10
feed.feedagent.collector.nodeip=192.168.33.10
feed.feedagent.collector.restartfreq=1800
#
# frequncy of feed channel health update
#
feed.feedagent.health.frequency=30
feed.feedagent.health.datafile=/var/cetas/feedhealth
#
# 0 = file based, 1 = ZK based
#
feed.feedagent.discover.method=1

#
# Crawler specific properties
#
crawler.file.incremental = false

#
# Parser specific properties
#
parser.detection.format.context.sensitive = true
parser.detection.format.unbuffered =
parser.detection.format.buffered.lines = 200
parser.detection.type.mode =
parser.detection.type.epoch = true
parser.detection.type.ipaddress =
parser.detection.type.phone =
parser.detection.type.thousands.separator =
parser.location.ipaddress =
parser.location.phone =
parser.attribute.undecorated =
parser.attribute.reserved.names.file = /opt/cetas/config/parser/reserved.names
parser.sampling.interval =
parser.sampling.s3 =
parser.empty.events.include =
parser.raw.event.index =
parser.raw.extraction.synchronous  =
parser.raw.extraction.asynchronous =
parser.raw.extraction.regex.list =
parser.raw.match.regex.list =
parser.raw.match.drop =
parser.report.events =
parser.report.time =
parser.report.data =
parser.dynamic.library.path = /opt/cetas/config/parser/dynamic
parser.pipeline.configuration.path = /opt/cetas/config/parser/pipeline
parser.allow.duplicates =
parser.date.patterns.file = /opt/cetas/config/parser/date.patterns
parser.date.patterns.list =
parser.format.name =
parser.format.dynamic.regex.file = /opt/cetas/config/parser/regular.expressions
parser.format.json.flat =
parser.format.log4j.patterns.file = /opt/cetas/config/parser/log4j.patterns
parser.format.log4j.patterns.list =
parser.format.log4j.timestamps.file = /opt/cetas/config/parser/log4j.timestamps
parser.format.log4j.timestamps.list =
parser.format.csv.strict =
parser.format.csv.file.header = true
parser.format.csv.header =
parser.format.csv.separator =
parser.format.csv.separator.list =
parser.format.csv.quotes.field =
parser.format.csv.quotes.enclosing =
parser.format.text.default = true
parser.format.text.header.file =
parser.format.text.tokenizer =
parser.format.text.index.raw =

#
# Data manager specific properties
#
manager.data.enable = true
manager.data.active.threads =
manager.data.notification.dequeuing.timeout = 5
manager.data.synchronous.messaging = true
manager.data.batch.size =
manager.data.sink.type =
manager.data.decompression.path = /parserdata
manager.data.duplicates.checked =
manager.data.retain.local =
manager.data.reporter =
manager.data.activity.reporting.interval = 10
manager.data.support.email.addresses =

#
# Timepivot specific details
#
userinterface.timepivot.charttype=tp
userinterface.timepivot.defaultdate=event_date
userinterface.timeseries.minimumdatapoints=10
userinterface.timeseries.maximumdatapoints=50
#
# Value of file size in KB for asynchronous upload
#
userinterface.upload.batchatsize=1024
userinterface.user.sessiontimeout=3600
userinterface.admin.tenantid=YWRtaW5AY2V0YXMubmV0
userinterface.searchcaching.enabled=true
userinterface.mgmt.api.caching.enabled=true

# Determain if Clustering Link should display in Analytics Dashboard
analytics.clusterlayout.enabled = false

#
# System Administrator Profile
#
management.sap.firstname=Cetas
management.sap.lastname=Eng
management.sap.email=
management.sap.smsid=
management.sap.emailcclist=
# Use following to send User registration e-mail alert
# More than one e-mail addresses should be separated with coma.
management.sap.registrationAlertEmail=

#
# Out going SMTP server information for sending e-mails
# for user registration activity & system alerts
#
# Using secured SMTP with authentication
#   - All property values need to be set below
#   - UserId, Password need to be encoded values, use mgmttools
#     to encode your ids
#   - Set auth=true
#
# Using internal open relay server (no authentication)
#   - SMTP host, port is required (usually port# is 25)
#   - UserId, Password parameters are ignored
#   - Set auth = false
#
management.mail.smtp.host=smtp.gmail.com
management.mail.smtp.port=465
management.mail.smtp.userid=0VmbuMXY0V2YAlHbwVmct8mb
management.mail.smtp.password=zITMzMjc052MzI3Z
management.mail.smtp.starttls.enable=true
management.mail.smtp.auth=true
management.mail.sender.name=Pivotal Analytics Support
management.mail.sender.mail=no-reply@gopivotal.com

#
# Proxy server setting, used for S3 access now.
#
management.proxy.host=proxy.vmware.com
management.proxy.port=3128

#
# The following property, when set to true, blocks the parser server
# (ParserQueueProcessor service) from reading any further messages
# from the notification queue. This is a cached global property, so
# its value can be changed via the PropertyUtil tool.
#
infrastructure.parser.server.maintenance = false

#
#following config will set null sink for REST server before
#sending data to AMQ
#
infrastructure.restserver.underMaintenance=false

#
# this flag will be used to indicate whether REST server will keep raw requests
#
infrastructure.restserver.retainmode=false

#
# The following property, when set to true
# syslog will be uploaded to s3 instead of sending to notification queue
infrastructure.livefeed.maintenance=false

#
# Vertical job setting
#
vertical.job.hdfs.cluster=localhost:9000
vertical.job.hadoop.cluster=
vertical.job.reducer.num=1

#
# Batch query facets value count limit
#
management.batch.facet.valuecount.limit=200

#
# Hand rails and Policy limits setting
#
# Project Number, per user
policy.free.total.project.num.limit=25

# Ingest each kind of datasource max size num limit, MB
policy.free.ingestion.per.crawl.source.max.size.limit=10240
policy.free.ingestion.per.upload.source.max.size.limit=100

# Ingest each kind of datasource num limit, per project per each method
policy.free.upload.num.limit=100
policy.free.ingestion.s3.num.limit=100
policy.free.ingestion.app.num.limit=5
policy.free.ingestion.syslog.num.limit=10

# Concurrent running job num limit, per user
policy.free.total.running.job.limit=10
policy.free.total.running.batch.job.limit=1

# Aggregate/Batch definition, per project
policy.free.dimension.num.limit=1000

# Aggregate definition, per user
policy.free.aggregate.num.limit=3
policy.free.date.num.limit.per.aggregate=3
policy.free.dimension.num.limit.per.aggregate=1000
policy.free.measure.num.limit.per.aggregate=5

# Batch definitionA, per user
policy.free.batch.def.num.limit=3
policy.free.date.num.limit.per.batch=10
policy.free.dimension.num.limit.per.batch=1000
policy.free.joins.num.limit.per.batch=3

# Storm manager related configurations
storm.killing.topology.when.shutdown=true
storm.topology.datapreview.deploy=true
storm.topology.streamquery.deploy=true

# Configure storm workers/tasks/executors for streaming
storm.streamingquery.workers=1
storm.streamingquery.dispatchbolt.executors=1
storm.streamingquery.searchbolt.executors=5
storm.streamingquery.searchbolt.tasks=10
storm.streamingquery.mergebolt.executors=1

#
# Auto Summarization parameters
#
autosummary.cube.slicing.max.distinct.values=100
autosummary.cube.slicing.max.distinct.values.percent=50
autosummary.cube.uniques.max.distinct.percent=60
autosummary.cube.slicing.include.source=true
autosummary.cube.min.events=10000
autosummary.cube.attribute.data.wait=600
autosummary.job.execution.interval=5
autosummary.cube.default.name=Summaries
autosummary.schema.idle.time=0
autosummary.lastjob.idle.time=0
autosummary.max.job.wait.time=120000
autosummary.max.num.attempts=3


# configuration for redis server
# we will put redis to scheduler node, replace to scheduler node IP
cache.redis.server.host=192.168.33.10
cache.redis.server.port=6379

#
# Auto Scale
#
autoscale.monitoring.url=http://ops.cetas.net:8000/zabbix/api_jsonrpc.php
autoscale.personalities.monitored=parser,messaging
autoscale.refresh.time=60000
autoscale.zk.server.override=
autoscale.mail.to=
autoscale.mail.cc=
infrastructure.svclocator.host=192.168.33.10
